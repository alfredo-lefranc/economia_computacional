---
title: "Targeted Marketing"
subtitle: "Tarea 4"
author: "Isidoro Garcia"
date: "2021"
output: 
  html_document:
    graphics: yes
    urlcolor: blue
    theme: spacelab
    df_print: paged
    toc: yes
    toc_depth: '3'
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      warning = FALSE,
                      fig.width = 6, fig.height = 4.5, fig.align = "right")
```

## Overview

Los contratan como data scientists para una empresa que vende electrodomesticos. La empresa lanzó un experimento de control aleatorio via un mail en donde se envió un catalogo de los productos al grupo de tratamiento `mailing_indicator`. 

Tu objetivo es estimar el impacto del envío sobre el gasto incremental: 

$$\tau_{i}=\mathbb{E}[Y_{i}(1)-Y_{i}(0)|\boldsymbol{x}_{i}],$$

En particular, queremos estimar el impacto de enviar el catalogo a nivel de cliente. Para ello, pondremos a competir algunos de los modelo de Causal Machine Learning que hemos aprendido en clase: 

- Double Debiased Machine Learning 

- Causal Forests 

Adicionalmente, desarrollen una estrategia de focalización con base en los resultados de tu modelo. Elabora sobre la lógica económica (i.e. identifica los Beneficios y Costos Marginales de enviar la campaña). Finalmente, corrobora la validez externa de la estrategia usando datos de un año. Esto nos dará un termómetro de la utilidad del modelo para campañas posteriores. 

Tip!: En los chunks donde vaya a haber modelos o cálculos complicados, usen `cache=T`

## Paso 1: Estimación y predicción the Conditional Average Treatment Effects (CATE)

Carguemos los datos de 2015

```{r}
library(tidyverse)
library(data.table)
library(gamlr)
library(grf)
library(xgboost)
library(ranger)
library(RCT)
library(lfe)
library(stargazer)
library(kableExtra)
library(knitr)
library(devtools)
library(parallel)
library(broom)
library(causalTree)




# desactiva notación científica       
options(scipen = 999)
```

```{r, cache=TRUE}
load("Customer-Development-2015-1.RData")
```

Dividimos la base en entrenamiento y validacion. Usamos un seed fijo para replicabilidad.

```{r}
set.seed(1990)
crm<-
  crm %>% 
  mutate(training_sample = rbinom(n = nrow(crm), 1, 0.7))
  
```


#### Data cleaning

1. Haz una primera revisión de la base. Cuantas variables tienen `NA`

```{r}
str(crm)

# parece que todas son numéricas

# nos devuelve las columnas que tengan NAs
(apply(crm, 2, function(x) sum(is.na(x))) > 0) %>% which()

# No hay NAs!!

```
Ninguna

2. Muestra la matriz de correlación entre variables. Muestra los pares de variables que tienen más de 95% de correlación. Remueve una de cada par multicolineal. 

```{r}
# mariz de correlacion
cor_mat <- cor(crm)

# triangulo superior de la matriz de correlacion
# asi tenemos cada combinacion entre variables 1 sola vez
# y tampoco tenemos la combinación de cada variable consigo misma (diagonal)
cor_mat[lower.tri(cor_mat, diag = TRUE)] <- NA

# dummy si la columna x tiene alguna correlación mayor a 0.95
cor_mat[,68] %>% abs() %>% max(na.rm = TRUE) > 0.95

# aplico la linea a toda la matriz  y extraigo los nombres de las columnas que
# cumplan con esto
highcor <- cor_mat[,(apply(cor_mat, 2, 
                          function(y) (abs(y) %>% 
                                         max(na.rm = TRUE) > 0.95)))] %>%
  colnames()

# variables a tirar
highcor

# filtramos la base
crm <- crm %>% select(-highcor)

```

3 (2 pts). Corrobora que la asignación tratamiento fue aleatoria mediante revisión del balance. Realiza las pruebas balance T y F. Cuántas variables salen desbalanceadas? Que muestra esto sobre la asignación de tratamiento?

```{r , eval= FALSE, cache=TRUE}
summary(factor(crm$mailing_indicator))

#82784
#167216
```


```{r balancet, eval= FALSE, cache=TRUE}
# pruebas t
balance_t<-balance_table(data = crm,
treatment = "mailing_indicator")
balance_t$p_value1 <- balance_t$p_value1 %>% round(3)
```


```{r, eval=FALSE}
#filtro

balance_t_filter<- balance_t %>% filter(p_value1 < .1)

#medias  
kable(balance_t_filter, digits = 3, align='c',
      booktabs=T, caption="Tabla de Balance")%>%
kable_styling(position = "center",latex_options="scale_down")

#34 desbalanceadas
```



```{r balancef, cache=TRUE}
# pruebas F
balance_f<-balance_regression(data = crm,
                              treatment = "mailing_indicator")

kable(balance_f$F_test, align = 'c',booktabs=T,digits = 2,
        caption = "Regresión de balance") %>%
  kable_styling(position = "center")
```

4. Realize un ajuste de False Discovery Rate al 10%. Cuántas variables salen desbalanceadas ahora? 

```{r fdr, cache=TRUE}
library(broom)

# funcion que realiza FDR
# inputs: un modelo guardado y una q entre (0,1)
get_fdr <- function(model,q){
  #Creo un dataframe con los pvalues del modelo
  pvalues<-as.data.frame(tidy(model))
  
  #Quito los pvalues que aparecen como NAs
  pvalues <- pvalues %>% filter(p.value != "NA") %>% 
    dplyr::select(term,p.value)
  
  
  # ordeno los regresores de acuerdo al p-value
  fdr <- pvalues %>% arrange(p.value) %>%
    mutate(indice = 1:length(p.value)) %>%
    mutate(`q*k/n` = q*(indice/length(p.value))) %>%
    mutate(cortes = p.value <= `q*k/n`)
  
  # Encontramos la p estrella (p-value de corte)
  pestrella <- fdr %>% filter(cortes == 'TRUE')
  pestrella <- max(pestrella$`q*k/n`)
  
  fdr <- fdr %>% dplyr::select(-cortes) %>%
            mutate(significant = p.value<=pestrella)
  fdr$p.value <- fdr$p.value %>% round(4)
  
  return(fdr)
}

# definir el modelo
reg <- lm(outcome_spend~.-mailing_indicator, data = crm)
fdr <- get_fdr(reg, 0.1)

# resultados del FDR
fdr %>% filter(significant) %>%
  kable()%>%
  kable_styling()

# 66 variables pasan el corte

# guardo las variables que pasan el corte menos el intercepto
goodvars <- fdr$term[fdr$significant==T & fdr$term!="(Intercept)"]

# filtro base
crm_2 <- crm %>% select(c("outcome_spend","mailing_indicator"),all_of(goodvars))
```

```{r balance2, eval=FALSE, cache=TRUE}
# repetir balance con la prueba t
balance_t2 <- balance_table(data = crm_2,
treatment = "mailing_indicator")
balance_t2$p_value1 <- balance_t2$p_value1 %>% round(3)

#filtro
balance_t2_filter<- balance_t2 %>% filter(p_value1 < .1)

# medias
kable(balance_t2_filter, digits = 3, align='c',
      booktabs=T, caption="Tabla de Balance")%>%
kable_styling(position = "center",latex_options="scale_down")

# 18 observaciones
# xq nos salieron menos? explicar
```


### Estimación de impacto de tratamiento (ATE)

5 (2pts). Estima el impacto promedio de enviar el catalogo vía email. Estima el impacto sin controles y luego agregar dos estimaciones de robustez: 1) Agregando variables que salieron significativas y 2) Agregando variables que salieron significativas con el FDR. Interpreta los resultados 

```{r}

naive_with_covariates<-paste("outcome_spend~mailing_indicator+", paste(balance_t2_filter$variables1, collapse=" +"))
noquote(naive_with_covariates)
naive<-lm(noquote(naive_with_covariates),crm)
```

```{r}
#modelo naive
naive<-lm(outcome_spend~mailing_indicator,crm)

#naive_controles
# solamente accedi a los covarites de manera automatica, no se preocupen, se actualiza solo ;)
text_naive_with_covariates<-paste("outcome_spend~mailing_indicator+", paste(balance_t_filter$variables1, collapse=" +"))
naive_covariates<-lm(noquote(text_naive_with_covariates),crm)

# naivee_controles_fdr

text_naive_with_covariates_fdr<-paste("outcome_spend~mailing_indicator+", paste(balance_t2_filter$variables1, collapse=" +"))
naive_covariates_fdr<-lm(noquote(text_naive_with_covariates_fdr),crm)

naive$coefficients[[2]]
naive_covariates$coefficients[[2]]
naive_covariates_fdr$coefficients[[2]]

#conclusion, agregar covariates solo hace nuestro estimador mas eficiente (en esepcial el de fdr)
```

#### Estimación de efectos heterogeneos

Usaremos el training sample para estimar el Conditional Average Treatment Effect de enviar el catalogo sobre el gasto en dólares. Estimaremos dos tipos de modelos (si agregan otro es bienvenido): 

(a) Double Debiased LASSO
(b) Causal Forests 


\bigskip

Separa la base de entrenamiento de la de validación



```{r validation}
set_validation_original <- crm[crm$training_sample==0,] %>%
  select(-c(outcome_spend,training_sample))

y_validation <- crm[crm$training_sample==0,] %>%
  select(outcome_spend)

set_training_original <- crm[crm$training_sample==1,] %>%
  dplyr::select(-c(training_sample))

```


```{r under}

# la muestra está balanceada? --> NO
set_training %>% group_by(mailing_indicator) %>%
  dplyr::summarise(frecuencia_absoluta=n()) %>% 
  mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)) %>%
  kable(col.names = c("Mailing_indicator",
                      "Frecuencia absoluta",
                      "Frecuencia relativa")) %>%
  kable_styling(position = "center",
                latex_options="HOLD_position")

# hacemos remuestreo para aumentar la eficiencia de nuestros modelos

# Undersampling. reducir tamaño de mailing_indicator==1
set_train_a <- sample_n((set_training %>% filter(mailing_indicator==1)),
                             size=(set_training %>% 
                                     filter(mailing_indicator==0) %>% 
                                     nrow()), 
         # tamaño de mailing_indicator==0
                             replace = FALSE) %>% 
  rbind((set_training %>% filter(mailing_indicator==0)),.)
```


####Double Debiased LASSO

6 (3pts). Estima un Double Debiased LASSO. Asegurate de mostrar el código. (Tip: recuerda que necesitas guardar el LASSO de cada K para poder usarlo en la base de validación)



```{r}
# dividir set de entrenamiento en k folds


# estratificamos por mailing_indicator para asegurar balance similar entre
# cada uno de los k folds y el set de entretamiento en promedio
k <- treatment_assign(data = set_training_original, share_control = 0.2,
                    n_t = 4, strata_varlist = "mailing_indicator",
                    missfits = "global",
                    seed = 1900, key = "customer_id")$data

k <- k %>% mutate(k = treat + 1) %>%
  ungroup()

# set_training <- bind_cols(set_training, k %>% select(k))

k <- k$k


# Lasso con cross fitting (en set de entrenamiento)

# separo variable de interés y de tratamiento
y_train <- set_training_original %>%
  select(outcome_spend)

trat_train <- set_training_original %>% select(mailing_indicator)

set_training <- set_training_original %>% select(-c(outcome_spend,mailing_indicator, customer_id))


# para correr modelos en paralelo
cores <- detectCores()
cl<-makeCluster(cores)
inicio<-Sys.time()

modelo <- NULL
modelo <- map_dfr(1:5, 
                function(a) {
                  treat_fit <-gamlr(x = set_training[k!=a, , drop= F],
                                    y = trat_train[k !=a],
                                    family="binomial")
                  
                  
                  
                  treat_hat <- as.numeric(predict(
                    treat_fit,
                    newdata = set_training[k==a, , drop= F],
                    type = "response"))
                  
                  spend_fit <- gamlr(x = set_training[k!=a, , drop= F],
                                    y = y_train[k !=a])
                  
                  spend_hat<-as.numeric(predict(
                    spend_fit,
                    newdata = set_training[k==a, , drop= F],
                    type = "response"))
                  
                  treat_resid <- trat_train[k==a] - treat_hat
                  spend_resid <- y_train[k==a]- spend_hat
                  

                  fits <- bind_cols("treat_hat" = treat_hat,
                                    "spend_hat"= spend_hat,
                                    "treat_resid" = treat_resid,
                                    "spend_resid" = spend_resid) %>%
                    as.data.frame()
                  

                  return(fits)
                  }
                )



# predicciones en set de validación

# separo variable de tratamiento
trat_validation <- set_validation_original %>% select(mailing_indicator)

set_validation <- set_validation_original %>% select(-c(mailing_indicator, customer_id))


# funcion para hacer la estimacion de T y spend en validacion


predics <- NULL
predics <- map_dfc(1:5, 
                function(a) {
                  
                  # LASSO tratamiento
                  treat_fit <-gamlr(x = set_training[k!=a, , drop= F],
                                    y = trat_train[k !=a],
                                    family="binomial")
                  
                  # estimacion de tratamiento
                  treat_hat <- as.numeric(predict(
                    treat_fit,
                    newdata = set_validation,
                    type = "response"))
                  
                  # LASSO spend
                  spend_fit <- gamlr(x = set_training[k!=a, , drop= F],
                                     y = y_train[k !=a])
                  
                  # estimación spend
                  spend_hat <- as.numeric(predict(
                    spend_fit,
                    newdata = set_validation,
                    type = "response"))
                  
                  # guardo los scores
                  fits <- bind_cols(treat_hat,
                                    spend_hat) %>%
                    as.data.frame()
                  
                  }
                )

(tiempo<-Sys.time() - inicio)
stopCluster(cl)

# nombro columnas de predics

# ALGUIEN SABE CÓMO METER ESTA LÍNEA A LA FUNCIÓN?

names(predics) <- c("treat_hat_1","spend_hat_1",
                    "treat_hat_2","spend_hat_2",
                    "treat_hat_3","spend_hat_3",
                    "treat_hat_4","spend_hat_4",
                    "treat_hat_5","spend_hat_5")

# promedio de los scores
predics$spend_hat <- (predics$spend_hat_1+
                      predics$spend_hat_2+
                      predics$spend_hat_3+
                      predics$spend_hat_4+
                      predics$spend_hat_5)/5

predics$treat_hat <- (predics$treat_hat_1+
                      predics$treat_hat_2+
                      predics$treat_hat_3+
                      predics$treat_hat_4+
                      predics$treat_hat_5)/5


```




7 (2pts). Cuál es el impacto de tratamiento promedio? Estimalo de dos maneras: 1) `spend_resid~treat_hat + treat` y 2) `spend~treat_resid`. Sale lo mismo? Justifica tu respuesta

```{r}

#_________________________
# ATE en el set de entrenamiento
#________________________

# primero agrego las variables spend y treat a la base con los residuales
modelo$spend <- y_train[[1]]
modelo$treat <- trat_train[[1]]
colnames(modelo)[3:4] <- c("treat_resid","spend_resid")

# regresion 1
spend_ate<-lm(spend_resid ~ treat_hat + treat,
data = modelo) %>%
tidy()

# ate 1
spend_ate$estimate[3]

# regresion 2
spend_ate2 <-lm(spend ~ treat_resid,
data = modelo) %>%
tidy()

# ate 2
spend_ate2$estimate[2]

#_________________________
# ATE en el set de validacion
#________________________

predics$spend <- y_validation[[1]]
predics$treat <- trat_validation[[1]] 

# residuales
predics$spend_resid <- predics$spend - predics$spend_hat
predics$treat_resid <- predics$treat - predics$treat_hat


# regresion 1
spend_atev <- lm(spend_resid ~ treat_hat + treat,
data = predics) %>%
tidy()


#_________________________
# IMPRIMO ATES
#________________________


# ate 1
spend_atev$estimate[3]

# regresion 2
spend_ate2v <-lm(spend ~ treat_resid,
data = predics) %>%
tidy()

# ate 2
spend_ate2v$estimate[2]

# prueba de igualdad
((spend_ate$estimate[3] - spend_ate2$estimate[2]) / sqrt(spend_ate$std.error[3]^2 + spend_ate2$std.error[2]^2) ) %>% pnorm()

((spend_atev$estimate[3] - spend_ate2v$estimate[2]) / sqrt(spend_atev$std.error[3]^2 + spend_ate2v$std.error[2]^2) ) %>% pnorm()

```
[1] 2.187591
[1] 2.022087
[1] 1
[1] 0.9938551

Los estimadores entre test y validación son diferentes...

Los valores estimados del ATE muestran diferencias menores a una décima, es decir, el impacto de enviar el catálogo a los clientes medido por una y otra forma varía por centavos. De hecho, no se puede rechazar la hipótesis de que estos coeficientes son iguales. Esto era de esperarse pues la relación entre variables dependientes y dependiente es la misma, sólo que invertida: la varianza explicada por todos los factores externos al tratamiento contra la variación en el tratamiento y el gasto, respectivamente. 


8 (3pts). Cuáles son las variables más importantes para las nuisance functions $T_i = g(X_i)+v_i$ y $y_i=m(X_i)+\epsilon_i$? (Tip: toma las variables que tengan $\beta \neq 0$ en cada $k$ y haz un `inner_join`. De ahí muestra el promedio de los coeficientes) Interpreta la función $g(X_i)$, porque sale así?

```{r}

coefs_treat <- map_dfr(1:5, 
                function(a) {
                  treat_fit <-gamlr(x = set_training[k!=a, , drop= F],
                                    y = trat_train[k !=a],
                                    family="binomial")
                  
                  coefs <- summary(coef(treat_fit)) %>% as.data.frame()
                  return(coefs)
                  }
                )

# la variable i considera al intercepto como 1 y a las demas variables en el orden
# del set_training

# corregimos esto para que coincidan
coefs_treat$i <- coefs_treat$i -1

# asignamos el nombre de la variable a cada coeficiente
coefs_treat$var <- ""
coefs_treat$var[coefs_treat$i==0] <- "Intercept"
coefs_treat$var[coefs_treat$i>0] <- colnames(set_training)[coefs_treat$i]

# promediamos los coeficientes
aggregate(coefs_treat$x, by=list(coefs_treat$var), mean) %>%
  kable(., col.names = c("Variable", "Coeficiente promedio"))
```


```{r}
# Se repite el procedimiento para el modelo de Y

coefs_spend <- map_dfr(1:5, 
                function(a) {
                  spend_fit <- gamlr(x = set_training[k!=a, , drop= F],
                                    y = y_train[k !=a])
                  
                  coefs <- summary(coef(spend_fit)) %>% as.data.frame()
                  return(coefs)
                  }
                )


# hacer coincidir i con los nombres de variables de la base
coefs_spend$i <- coefs_spend$i -1

# asignamos el nombre de la variable a cada coeficiente
coefs_spend$var <- ""
coefs_spend$var[coefs_spend$i==0] <- "Intercept"
coefs_spend$var[coefs_spend$i>0] <- colnames(set_training)[coefs_spend$i]

# promediamos los coeficientes
aggregate(coefs_spend$x, by=list(coefs_spend$var), mean) %>%
  kable(., col.names = c("Variable", "Coeficiente promedio"))
```

Cuando el tratamiento se asigna de forma aleatoria exitosamente, es de esperarse que pocas variables salgan significativas en la función $g(X_I)$, y que su poder predictivo sea muy pequeño.


9 (3pts). Ahora corre un DDML LASSO para encontrar los efectos a nivel cliente (Tip: interactúa todas las variables con `treat_resid`. Muestra el código. Qué varaibles salen significativas?

```{r}
# creo bases de interacciones
W_train <- modelo$treat_resid*set_training


# le cambio el nombre a estas variables para distinguirlas de las no interactuadas
colnames(W_train) <- paste0("treat:",colnames(W_train))


# Corro LAsso en la base de entrenamiento
ddhte_train <- gamlr(x= cbind(set_training,modelo$treat_resid,W_train),
               y = modelo$spend_resid)
  
signifs_train <- summary(coef(ddhte_train)) %>% as.data.frame()


colnames(cbind(set_training,modelo$treat_resid,W_train))
# hacer coincidir i con los nombres de variables de la base
signifs_train$i <- signifs_train$i - 1

# asignamos el nombre de la variable a cada coeficiente
signifs_train$var <- ""
signifs_train$var[signifs_train$i==0] <- "Intercept"
signifs_train$var[signifs_train$i>0] <- 
  colnames(cbind(set_training,modelo$treat_resid,W_train))[signifs_train$i]

signifs_train %>% select(c(var,x)) %>%
  kable(., col.names = c("Variable", "Coeficiente"))

# aquí sería bueno ordenar las filas por relevancia del coeficiente y 
# cambiarle el nombre a trear_resid que sale como V2.

# igual en la tabal de abajo
```

```{r}

#duda aqui no hay que estimar otra vez sino correr creo que este chunk es inecesario
W_test <- predics$treat_resid*set_validation
colnames(W_test) <- paste0("treat:",colnames(W_test))


# Corro lasso en la base de validación
ddhte_test <- gamlr(x= cbind(set_validation,predics$treat_resid,W_test),
               y = predics$spend_resid)

signifs_test <- summary(coef(ddhte_test)) %>% as.data.frame()


# hacer coincidir i con los nombres de variables de la base
signifs_test$i <- signifs_test$i - 1

# asignamos el nombre de la variable a cada coeficiente
signifs_test$var <- ""
signifs_test$var[signifs_test$i==0] <- "Intercept"
signifs_test$var[signifs_test$i>0] <- 
  colnames(cbind(set_validation,predics$treat_resid,W_test))[signifs_test$i]

signifs_test %>% select(c(var,x)) %>%
  kable(., col.names = c("Variable", "Coeficiente"))
```

10 (2 pts). Predice el CATE en la base de entrenamiento y en la base de validación. Como se ve la distribución del impacto de tratamiento en ambas? 


Supongo que el CATE es el efecto a nivel cliente que se pide en la pregunta anterior.

# Efecto a nivel cliente

Supongo que, como se interactuó el efecto de tratamiento con todas las X, el impacto de tratamiento depende de los niveles de x de cada cliente. Por lo tanto, $\tau_i$ es igual al ATE más todas las interacciones significativas del ATE con las X a los niveles de cada individuo.

Una forma de medirlo es hacer un predict con el modelo completo, que es $E(spend_i | \tau_i=1, X_i)$ y restarle $E(spend_i | \tau_i=0,X_i)$. 

```{r}
#OSEA CALCULAR EL CONTRAFACTUAL y luego comparar para cada i real vs contrafactual
#SE ME OCURRE PREDECIR EL EFECTO PARA LA BASE DE DATOS CON LA T INVERTIDA (SI ES 1 LA CAMBIO A 0 Y VICEVERSA)


#predicciones normales
W_test <- predics$treat_resid*set_validation
colnames(W_test) <- paste0("treat:",colnames(W_test))

test<-cbind(set_validation,predics$treat_resid,W_test)
pred_normales<-predict(ddhte_train,newdata = test) #predicciones
obs <-as.data.frame(as.matrix(pred_normales))


#contrafactual

#primero creo una variable alrevezx
trat_validation_counterfactual<-trat_validation%>%mutate(mailing_indicator=ifelse(mailing_indicator==1,0,1))
predics$treat_counter <- trat_validation_counterfactual[[1]] 
# residuales alrevez (con ts volteados)
predics$treat_counter_resid <- predics$treat_counter - predics$treat_hat
#repito lo mismo
W_test_counter <- predics$treat_counter_resid*set_validation
colnames(W_test_counter) <- paste0("treat:",colnames(W_test_counter))
test_counter<-cbind(set_validation,predics$treat_counter_resid,W_test_counter)
pred_contraf<-predict(ddhte_train,newdata = test_counter) #predicciones
contraf <- as.data.frame(as.matrix(pred_contraf))


#ahora puro callo para hacer la resta de neyman pero ahora tratamiento - control 
#para cada i 
cates<-cbind(obs,contraf,predics$treat,predics$treat_counter)
colnames(cates) <- c("obs","contraf","treat","treat_contraf")
cates<-cates %>% mutate(a=obs*treat+contraf*treat_contraf,
                       b=obs*treat_contraf+contraf*treat,
                       cate=a-b )

#plot
ggplot(cates, aes(x=cate)) + geom_histogram(binwidth=1)+geom_vline(aes(xintercept=0),
            color="red", linetype="dashed", size=.5)+xlim(-10,30)

```


####Causal Forest

11 (2pts). Ahora vayamos al causal forest. Estima un causal forest en la base de entrenamiento (Estima 750 árboles)

```{r, cache = FALSE}
# CAUSAL FOREST

tree_formula <- as.formula(paste("outcome_spend",paste(names(crm %>% dplyr::select(-c(outcome_spend,mailing_indicator))),collapse = ' + '), sep = " ~ "))

no_covariates <- c("outcome_spend", 
                     "mailing_indicator",
                     "customer_id")

                     #"training_sample")

matrix <- as.matrix(set_training_original %>% dplyr::select(-no_covariates))

cf <- causal_forest(
  X = matrix,
  Y = set_training_original$outcome_spend,
  W = set_training_original$mailing_indicator,
  num.trees = 750
)

# Comprobemos que el modelo está bien calibrado, es decir que este acotado lejos del 0 y del 1. Además se cumple el Overlap como podemos ver

ggplot(data.frame(W.hat = cf$W.hat, W=factor(cf$W.orig)))+
  geom_histogram(aes(x = W.hat, y = stat(density), fill= W ), alpha = 0.3, position = "identity")+
  geom_density(aes(x=W.hat,color=W))+
  labs(title="Causal Forest Propensity Scores",
       caption="Propensity scores entrenados via un GRF")+
  theme_bw()



```

12 (3pts). Cómo se distribuye el impacto de tratamiento? Cuál es el impacto de tratamiento (ATE)? Qué tanto se acerca al impacto de tratamiento "real"? Cómo se compara con el impacto estimado con el ddml simple?

```{r}

# Veamos las predicciones y cómo se distribuyen
# sacamos las predicciones IN SAMPLE


predicciones_is <- predict(cf ,estimate.variance = TRUE)

tau_hat_cf_is <- predicciones_is$predictions

ggplot(data=predicciones_is)+
  geom_histogram(aes(x = tau_hat_cf_is, y = stat(density)), position = "identity")+
  geom_density(aes(x=tau_hat_cf_is, y=stat(density)))+
  xlim(-20,20)+
  theme_bw()+
  labs(title="Causal Forest: insample CATE",
       caption="En la gráfica se omiten predicciones mayores a 20 y menores a -20")


# Obtenemos un CATE de
mean(tau_hat_cf_is)

# Como veremos a continuación, el ATE se acerca mucho al impacto de tratamiento "real"

#  podemos sacar el conditional average treatment efecto "real"
average_treatment_effect(
  cf,
  target.sample=c("all"),
  method=c("AIPW"),
  subset=NULL
)

```

```{r}
# sacamos las predicciones OUT OF SAMPLE


predicciones_out <- predict(cf ,  newdata = set_validation,estimate.variance = TRUE)

tau_hat_cf_out <- predicciones_out$predictions

ggplot(data=predicciones_out)+
  geom_histogram(aes(x = tau_hat_cf_out, y = stat(density)), position = "identity")+
  geom_density(aes(x=tau_hat_cf_out, y=stat(density)))+
  xlim(-20,20)+
  theme_bw()+
  labs(title="Causal Forest: out-of-bag CATE",
       caption="En la gráfica se omiten predicciones mayores a 20 y menores a -20")


# Obtenemos un CATE de
mean(tau_hat_cf_out)

# Como veremos a continuación, el ATE se acerca mucho al impacto de tratamiento "real"

#  podemos sacar el conditional average treatment efecto "real"
average_treatment_effect(
  cf,
  target.sample=c("all"),
  method=c("AIPW"),
  subset=NULL
)


```

```{r, cache=FALSE, include=FALSE}

# !!!! OJO
#CAUSAL TREE (ES OPCIONAL, para quien le interese)
####################################################


#1.- usamos nuestra base de entrenamiento
df_training <- crm %>% 
filter(training_sample==1)


#2.- Dividimos entrenamiento entre Splitting y Estimating

# tamaño del split
split_size <- floor(nrow(df_training)*0.5)

# aleatoriamente generamos el split
split_id <- sample(nrow(df_training),replace=FALSE, size=split_size)

# hacemos el split
df_split <- df_training[split_id,] 
df_est <- df_training[-split_id]

# Ahora sí, el árbol:

tree_formula <- as.formula(paste("outcome_spend",paste(names(crm %>% select(-no_covariates)),collapse = ' + '), sep = " ~ "))

ctree_unpruned <- honest.causalTree(
  formula=tree_formula,
  data=df_split,
  est_data=df_est,
  treatment=df_split$mailing_indicator,       # variable de tratamiento
  est_treatment=df_est$mailing_indicator,
  split.Rule="CT",
  cv.option = "TOT",
  cp=0,
  split.Honest = TRUE,
  cv.Honest=TRUE,
  minsize=5,
  HonestSampleSize = nrow(df_est)
)


# Ahora hacemos crossvalidate
ctree_cptable <- as.data.frame(ctree_unpruned$cptable)

# Obtenemos parámetro óptimo de complejidad para podar el árbol
selected_cp <- which.min(ctree_cptable$xerror)
optim_cp_ct <- ctree_cptable[selected_cp, "CP"]

# podamos el árbol
ctree_pruned <- prune(tree=ctree_unpruned, cp=optim_cp_ct)

# dibujemos el árbol


# predecimos estimaciones puntuales en la estimation sample
tauhat_causaltree_est <- predict(ctree_pruned, newdata=df_est)

# saquemos errore estándar
# el paquete no los saca

num_hojas <- length(unique(tauhat_causaltree_est))

```


13. Haz un scatter plot de las predicciones de ambos modelos? Hay alguna relación?

```{r}
# IN SAMPLE
tau_hat_ddbl_is<-(modelo$spend_hat)
data_is <- as.data.frame(tau_hat_cf_is,tau_hat_ddbl_is)

ggplot(data_is, aes(x=tau_hat_cf_is, y=tau_hat_ddbl_is)) +
  geom_point()+xlim(-5,100)+ylim(-5,100)

```



```{r}
# OUT OF SAMPLE
tau_hat_ddbl_out<-(predics$spend_hat)
data_out <- as.data.frame(tau_hat_cf_out,tau_hat_ddbl_out)

ggplot(data_out, aes(x=tau_hat_cf_out, y=tau_hat_ddbl_out)) +
  geom_point()+xlim(-5,100)+ylim(-5,100)
```


14 (4pts). Evalúa el poder predictivo de cada modelo (OOS). Esto se hace por modelo: Divide la muestra en 10 partes con base en el score de ddml. Para cada parte, estima el impacto de tratamiento vía una regresión y saca el promedio del score. Valida si para los grupos que dice el score el impacto será más grande, el coeficiente de la regresión es. Cómo se ven los modelos? Cuál parece ser mejor?

```{r}
#primero dividir set validacion 

library(gtools)
g <- quantcut(predics$spend_hat, c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9, 1))
datas<-split(predics, g)

#estima impacto de tratamiento con una regresion y saca el promedio del score NOSABEMOS :(
#duda, 

fitLM <- function(x){
lm1 <- lm(x$spend~x$treat,data = x)
return(summary(lm1))
}
regresiones <- lapply(datas,FUN=fitLM)


chart_ddbl <- as.data.frame(matrix(0, 1, 10))

for (i in 1:10) {
chart_ddbl[,i]<-tidy(regresiones[[i]])[2,2]}

# validar si grupos con score el impacto es mas grande
#masomenos

```

```{r}
#para rf
#primero dividir set validacion 

predicciones<-cbind(predicciones_out,predics$treat,predics$spend)
colnames(predicciones) <- c("score","var","treat","spend")
predicciones<-as.data.frame(predicciones)


g <- quantcut(predicciones$score, c(0,.1,.2,.3,.4,.5,.6,.7,.8,.9, 1))
datas<-split(predicciones, g)

#estima impacto de tratamiento con una regresion y saca el promedio del score NOSABEMOS :(
#duda, 

regresiones <- lapply(datas,FUN=fitLM)

chart_rf <- as.data.frame(matrix(0, 1, 10))

for (i in 1:10) {
chart_rf[,i]<-tidy(regresiones[[i]])[2,2]}

# validar si grupos con score el impacto es mas grande
#masomenos

```


15 (6 pts). Construye una estrategia de focalización a nivel usuario con base a los resultados de cada modelo. Considera lo siguiente: 

- El costo marginal de mandar el mail es 0.99 USD

fijo y no hay pex es .99 por cada mail....
 
 
- El Beneficio marginal es el impacto incremental la utilidad generada por esas ventas

beneficio marginal  = spend- costo del mail

- El margen de ganancia sobre las ventas es de 32.5 fijo 

¿cual es la diferencia entre margen de ganancia beneficio marginal? margen de 

ganacial=¿precio-costo?=32.5





Con esto, indica: 

- Cuantos usuarios entrarían a la campaña? 

caluclau en cuantos el beneficio > 0

- A partir de cuánto lift (ventas incrementales) entran? 

de la poblacion anterior calcular su lift (el treshold del modelo)



- Cuál es el impacto promedio esperado de tu población final? 

suma de beneficios totales/personas participantes

- Cuánta utilidad haremos con esta estrategia? Cómo se compara con la utilidad de la campaña sin focalizar?

suma de beneficios totales

resta de beneficios totales con campaña menos beneficios totales sin campaña



16 (3pts). Haz una gráfica del la utilidad total vs q (personas que entran en la campaña) para DDML y CF



