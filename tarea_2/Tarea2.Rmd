---
output:
  pdf_document: null
  geometry: margin=1in
  html_document:
    df_print: paged
fontsize: 11pt
header-includes: \usepackage{geometry} \usepackage{graphicx} \tolerance=1 \hyphenpenalty=10000
  \hbadness=10000 \linespread{1.2} \usepackage[justification=centering, font=bf, labelsep=period,
  skip=5pt]{caption} 
  \usepackage{babel} \usepackage{fancyhdr}
  \pagestyle{fancy} \fancyhead[L]{Maestría en Economía Aplicada} \fancyhead[R]{ITAM}
---
\begin{titlepage}
\begin{center}

\textsc{\Large Instituto Tecnológico Autónomo de México}\\[2em]

\textbf{\LARGE Economía Computacional}\\[2em]


\textsc{\LARGE }\\[1em]


\textsc{\LARGE Tarea 2 }\\[1em]
\textsc{\large }\\[1em]
\textsc{\LARGE Equipo 7 }\\[1em]


\textsc{\large }\\[1em]
\textsc{\LARGE }\\[1em]


\textsc{\large }\\[1em]
\textsc{\LARGE }\\[1em]

\textsc{\LARGE Prof. Isidoro García Urquieta}\\[1em]

\textsc{\LARGE }\\[1em]
\textsc{\LARGE }\\[1em]

\textsc{\LARGE Alfredo Lefranc Flores}\\[1em]

\textsc{\large 144346}\\[1em]

\textsc{\LARGE Cynthia Raquel Valdivia Tirado }\\[1em]

\textsc{\large 81358}\\[1em]

\textsc{\LARGE Rafael Sandoval Fernández}\\[1em]

\textsc{\large 143689}\\[1em]

\textsc{\LARGE Marco Antonio Ramos Juárez}\\[1em]

\textsc{\large 142244}\\[1em]

\textsc{\LARGE Francisco Velazquez Guadarrama}\\[1em]

\textsc{\large 175606}\\[1em]

\end{center}

\vspace*{\fill}
\textsc{Ciudad de México \hspace*{\fill} 2021}

\end{titlepage}


\newpage


\tableofcontents

\newpage

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE, warning=FALSE,
                      fig.width = 7, fig.height = 4, fig.align = "right")
```

```{r, message=FALSE}
packages <- c(
  "tidyverse",
  "data.table",
  'broom',
  'knitr',
  'lubridate',
  'RCT',
  'gamlr',
  'ranger',
  'tree',
  'parallel',
  'naniar',
  'kableExtra',
  'gridExtra',
  'VIM',
  'smotefamily',
  'stargazer',
  'xgboost',
  'DiagrammeR',
  'CustomerScoringMetrics'
  )

# instala los que no tengas
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# cargar paquetes
lapply(packages,
       library,
       character.only = TRUE)

# funciones

check_nas <- function(df){
  df %>%
    select_if(~sum(is.na(.)) > 0) %>%
    miss_var_summary()
}

```



## Contexto

Cell2Cell es una compañía de teléfonos celulares que intenta mitigar el abandono de sus usuarios. Te contratan para 1) Encontrar un modelo que prediga el abandono con acierto y para usar los insights de este modelo para proponer una estrategia de manejo de abandono.


Las preguntas que contestaremos son:

1. Se puede predecir el abandono con los datos que nos compartieron?

2. Cuáles son las variables que explican en mayor medida el abandono?

3. Qué incentivos da Cell2Cell a sus usarios para prevenir el abandono?

4. Cuál es el valor de una estrategia de prevención de abandono focalizada y cómo difiere entre los segmentos de los usuarios? Qué usuarios deberían de recibir incentivos de prevención? Qué montos de incentivos

Nota: Voy a evaluar las tareas con base en la respuesta a cada pregunta. Como hay algunas preguntas que no tienen una respuesta clara, al final ponderaré de acuerdo al poder predictivo de su modelo vs las respuestas sugeridas.



\newpage

## Datos

Los datos los pueden encontrar en `Cell2Cell.Rdata`. En el archivo `Cell2Cell-Database-Documentation.xlsx` pueden encontrar documentación de la base de datos.

Carguemos los datos
```{r }
load('Cell2Cell.Rdata') %>% as.data.frame()
# file path de archivo
# path_data <- file.path('C:/Users/rsf94/Documents/economia_computacional/tarea_2',
#                       'Cell2Cell.RData')

# renombrar como data
data <- as.data.frame(cell2cell)
rm(cell2cell)
```

### 1. Qué variables tienen missing values? Toma alguna decisión con los missing values. Justifica tu respuesta

## Análisis de missing values general

Primero revisamos las columnas que tienen missing values y su cantidad.

```{r,results='asis'}
# Rafa
# tabla resumen de missing values
kable(check_nas(data),booktabs=T, align = 'c',
      col.names = c("Variale", "Cantidad","%"),digits = 2)%>%
  kable_styling(position = "center",
                latex_options="HOLD_position")
```

Revisamos más a detalle estas variables y notamos que todas son numéricas.

```{r}
# columnas con NAs
cols_con_nas <- names(which(colSums(is.na(data)) > 0))
df_nas <- data %>% select(all_of(cols_con_nas)) %>% as.data.frame()
summary(df_nas)
```

## NO SE SI QUITAR O DEJAR ESTO
Otro elemento importante a tener en cuenta son las coincidencias de los NAs en observaciones. Si hay una alta coincidencia, lo mejor es tirar esas observaciones.

```{r}

# numero de filas con NAs
data[!complete.cases(data),] %>% nrow()

# correlaciones entre variables con NAs
x <- as.data.frame(abs(is.na(data)))
# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)]
# la sd va a ser positiva para variables que tengan NAs

# Da la correación un valor alto positivo significa que desaparecen juntas.
cor(y)

# Visualización de los NAs
#aggr(data, plot=FALSE, prop=FALSE, numbers=TRUE)

```


Las 216 valores faltantes para revenue, mou, recchrge, directas, overage y roam coinciden.Lo mismo pasa con los 502 valores faltantes de changem y changer, con el valor faltante de phones, models, eqpdays, y con los 1244 valores faltantes de age1 y age2. Además, se puede ver en la matriz que los valores faltantes del primer grupo de variables coincide en un 65% con los valores faltantes de changem y changer. Para estos casos, se tiene valores faltantes en 8 de 68 variables, el 11.7% de las columnas.

Esto en general nos dice que no hay un problema serio de NAs. No obstante, para no perder dicha información, lo mejor sería hacer imputación.

Podemos imputar alguna medida de tendencia central a aquellas variables que parezcan tener una distribución normal o estén concentradas en uno o pocos valores. Para decidir esto, examinamos la distribución de estas variables.

```{r, warning=FALSE}
myhist <- function(yvar){
  ggplot(df_nas, aes_(x=as.name(yvar)))+
    geom_histogram()+
    ggtitle(paste0(as.name(yvar)))+
    xlab("")+
    ylab("")+
    theme(axis.text.y = element_blank())
}
hists<- df_nas %>%
        names() %>%
        lapply(myhist)


grid.arrange(grobs=hists,ncol=4)
```

## Analisis por variable

**age_1 y age_2**

Se trata de variables de edad que comienzan a partir de los 18 y para las cuales existe una etiqueta "0" que curiosamente es la moda. Es decir, no conocemos la edad para una gran parte de las observaciones. Podemos aprovechar esta etiqueta y extenderla para tratar los missing values, de tal manera que ahora los NA's tienen la etiqueta "0".
```{r}
summary(factor(data$age1))
summary(factor(data$age2))
data$age1[is.na(data$age1)] <- 0
data$age2[is.na(data$age2)] <- 0
```

**phones, models y eqpdays**

En este caso, solo estamos hablando de un missing value por lo que haremos algo sencillo, imputar la mediana..
```{r}
summary(factor(data$phones))
summary(factor(data$models))
summary((data$eqpdays))

data$phones[is.na(data$phones)] <- median(data$phones[!is.na(data$phones)]) %>% as.numeric
data$models[is.na(data$models)] <- median(data$models[!is.na(data$models)]) %>% as.numeric
data$eqpdays[is.na(data$eqpdays)]<- median(data$eqpdays[!is.na(data$eqpdays)])
```

**changem y changer**
```{r}
hist(data$changem)
hist(data$changer)
```

En este caso, ante la sospecha de posibles outliers decidimos imputar con la mediana muestral.

```{r}
data$changem[is.na(data$changem)] <- median((data$changem[!is.na(data$changem)]))
data$changer[is.na(data$changer)] <- median((data$changer[!is.na(data$changer)]))
```

**revenue, mou, recchrge, directas, overage, roam**

La primera sospecha que tenemos es que todas estas variables tienen las mismas observaciones como faltantes. Lo corroboramos con la siguiente exploración:

```{r}
inspeccion<-data %>% select(revenue, mou, recchrge, directas, overage, roam)
inspeccion[!complete.cases(inspeccion),] %>% nrow()
```

Esto quiere decir que sería inadecuado hacer imputaciones con base en información entre ellas, pues desconocemos todo. Lo que podemos hacer es A) imputar simplemente la media muestral o B) encontrar alguna relación con otras variables que nos permitan imputar con base en un modelo lineal sencillo.


```{r}
aux_data <- data.frame(sapply(data, function(x) as.numeric(as.character(x))))
aux_data<-aux_data[complete.cases(aux_data),]
cor_aux<-cor(aux_data)
cor_aux<-data.frame(cor_aux) %>% select (churn,changer,changem,revenue, mou, recchrge, directas, overage, roam)
```

En el data frame cor_aux podemos ver las correlaciones entre nuestras variables conmissing values y las demás. Podemos proponer imputar en cada variable el valor predicho por una regresión, De esta manera, lograremos una mejor imputación que con solo la media muestral. Para construir los modelos simplemente elegimos para cada variable las variables más correlacionadas (tanto negativa como positivamente), sin contar revenue, mou, recchrge, directas, overage ni roam pues los missing values son compartidos y no contamos con esa información para estimar.

```{r}
imp_revenue<-lm(revenue~peakvce+mourec,data)
imp_mou<-lm(mou~peakvce+opeakvce+mourec+outcalls+unansvce+callwait,data)
imp_recchrge<-lm(recchrge~peakvce+mourec+outcalls,data)
imp_directas<-lm(directas~peakvce+opeakvce+mourec+outcalls+callwait,data)
imp_overage<-lm(overage~peakvce+opeakvce+mourec+outcalls+callwait,data)
imp_roam<-lm(roam~peakvce+opeakvce+mourec+outcalls+callwait,data)
```

```{r}
stargazer(imp_revenue,imp_mou,imp_recchrge,imp_directas,imp_overage,imp_overage,imp_roam,type="text")
```

En esta tabla podemos observar que en general los  modelos nos dan una predicción mejor a la media, excepto en el ultimo modelo sobre roam. Esta información nos da luz verde para realizar una imputación con base en una regresión lineal.


```{r}
#imputo la predicción de la regresión
data<-data %>% mutate(revenue=ifelse(is.na(revenue),predict(imp_revenue,newdata=data),revenue))%>%
       mutate(mou=ifelse(is.na(mou),predict(imp_mou,newdata=data),mou))%>%
       mutate(recchrge=ifelse(is.na(recchrge),predict(imp_recchrge,newdata=data),recchrge))%>%
       mutate(directas=ifelse(is.na(directas),predict(imp_directas,newdata=data),directas))%>%
       mutate(overage=ifelse(is.na(overage),predict(imp_overage,newdata=data),overage))

#imputo la predicción la media mmuestral
data$roam[is.na(data$roam)] <- mean(data$roam[!is.na(data$roam)])
```

Finalmente checo mi base:


```{r,results='asis'}
cols_con_nas <- names(which(colSums(is.na(data)) > 0))
df_nas <- data %>% select(all_of(cols_con_nas)) %>% as.data.frame()
summary(df_nas)
```


```{r}
str(data)
# numero de filas con NAs
data[!complete.cases(data),]
```

### 2. Tabula la distribución de la variable `churn`. Muestra la frecuencia absoluta y relativa. Crees que se debe hacer oversampling/undersamping?
```{r, out.width = '70%',fig.align = "center"}

# tabulación
tabulado <- data %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
  mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta))

kable(tabulado,booktabs=T, align = 'c',
      col.names = c("Churn", "Cantidad","%"),digits = 2)%>%
  kable_styling(position = "center",
                latex_options="HOLD_position")

rm(tabulado)


data$churn <- as.numeric(data$churn)
# frecuencia absoluta
ggplot(data, aes(x=churn))+
  geom_bar(aes(y=..count..,fill=(factor(..x..)),stat="count"))+
  geom_text(aes(label= scales::number(..count..),
                y=..count..),stat="count", vjust=-0.5)+
  labs(title="Distribución de churn",
    subtitle="Frecuencia absoluta",
    fill="Churn")+
  ylab("Frecuencia absoluta")+
  scale_y_continuous(labels=scales::comma,
                     limits=c(0,60000))+
  scale_x_continuous(breaks=c(0,1))+
  theme_bw()+
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

# frecuencia relativa
ggplot(data, aes(x=churn))+
  geom_bar(aes(y=..prop.., fill=factor(..x..)),stat="count")+
    geom_text(aes(label= scales::percent(..prop..),
                y=..prop..),stat="count", vjust=-0.5)+
  scale_y_continuous(breaks=seq(0,1, by =0.1),
                     limits=c(0,1))+
  scale_x_continuous(breaks=c(0,1))+

    labs(title="Distribución de churn",
       subtitle="Frecuencia relativa",
       fill="Churn")+
  ylab("Frecuencia relativa")+
  theme_bw()+
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

```

Sí, parece que sí se debe hacer algún tipo de remuestreo para tener una base balanceada y que los modelos a estimar tengan mayor poder predictivo.


### 3. (2 pts) Divide tu base en entrenamiento y validación (80/20). Además, considera hacer oversampling (SMOTE) o undersampling. (Tip: Recuerda que el objetivo final es tener muestra ~balanceada en el traning set. En el validation la distribución debe ser la original)

Primero dividimos la base.
```{r}
set.seed(123)

# proporción que queremos de training
training_size <- 0.8

# filas de training
training_rows <- sample(seq_len(nrow(data)),size=floor(training_size*nrow(data)))

#training set
data_training <- data[training_rows,]

#validation set. guardamos la base de características y la variable objetivo por separado
data_validation <- data[-training_rows,-2]
churn_validation <- data[-training_rows,2]

# la muestra está balanceada? --> NO :(
(tabulado <- data_training %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
  mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))
```
No está balanceada, aunque sí conserva la proporción de clases de la base original. Procedemos a rebalancear, probamos 3 estrategias: undersampling, oversampling o una mezcla de ambas. A continuación mostramos cómo realizamos las tres estrategias sobre nuestra base de entrenamiento.

#### A. Undersampling

```{r undersampling}
# tamaño de churn==1 en la base de entrenamiento
freq_churn <- data_training %>% filter(churn==1) %>% nrow()


#simplemente reducimos la clase más abundante
undersampling_c0<-  sample_n((data_training %>% filter(churn==0)),
                             size=freq_churn,
                             replace = FALSE)

undersampling<-rbind((data_training %>%
                        filter(churn==1)),undersampling_c0)



```

#### B. oversampling

```{r oversampling, eval=FALSE}
# Segundo intento, sugiere hacer Synthetic Minority Oversampling Technique (SMOTE) librería smotefamily
# echénle un ojo a: https://rikunert.com/SMOTE_explained
smote <- smotefamily::SMOTE((data_training)[,-2], # data
                            data_training$churn, # class attribute
                            K=10, # number of nearest neighbors
                            dup_size=2 # desired times of synthetic minority instances
                            # over original number of majority instances
                            )


class(data %>% na.exclude)
oversampled <- as.data.frame(smote$data)

# smote guarda la variable de clase como class. se renombra y convierte a numerica
colnames(oversampled)[68] <- "churn"
oversampled$churn <- oversampled$churn %>% as.numeric

# sí está balanceado, aún no entiendo bien los outputs de la función
(tabulado <- oversampled %>% group_by(churn) %>%
    dplyr::summarise(frecuencia_absoluta=n()) %>%
    mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))


# $data arroja el data frame original + observaciones sintéticas de la minoría
length(as.data.frame(smote$data)$churn)

# syn_data arroja observaciones sintéticas de la minoría
length(as.data.frame(smote$syn_data)$churn)

# resta de ambos nos da el original
length((data_training %>% na.exclude)$churn)


```

#### C. Under y over sampling
```{r under-oversampling, eval=FALSE}

#para rebalancear de manera más precisa el oversampling, simplemente "podamos" las observaciones sinteticas de tal manera que nos quede una base de datos perfectamente balanceada.

freq_churn_os <- oversampled %>% filter(churn==0) %>% nrow()

#podamos
undersampling_c1<-  sample_n((oversampled %>% filter(churn==1)),
                             size=freq_churn_os,replace = FALSE)

#armamos la base
under_over_sampling<-rbind((oversampled %>% filter(churn==0)),undersampling_c1)

#listo!
(tabulado <- under_over_sampling %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
    mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))
```


De esta manera quedan 3 bases para comparar: undersampling, oversampling y una combinación de ambos. Comparamos las tres bases con los tres modelos y obtuvimos los mejores resultados de la base con undersampling. En adelante, mostramos nuestros resultados con esa base. En el script `remuestreo_alt.R` incluimos algunos resultados con las otras bases.


```{r new training}
data_training_a<-undersampling %>% na.exclude
```



## Model estimation

Pondremos a competir 3 modelos:

1. Cross-Validated LASSO-logit

2. Prune Trees

3. Random Forest

### 4 (2 pts). Estima un cross validated LASSO. Muestra el la gráfica de CV Binomial Deviance vs Complejidad
```{r}

#X <- sparse.model.matrix(~.+0, data = data_training[,-c(1,2)]) # Transformar a sparse matrix la información relevante

#cv_lasso <- cv.gamlr(x = X, y = data_training$churn, family = 'binomial', nfold = 5, verb = TRUE) # Estimar el CV LASSO

#par(mar=c(5,4,4,2) + 0.1)
#plot(cv_lasso) # Gráfico


##A

#Matriz de covariates
Xa <-data_training_a %>% select(-customer,-churn)

#se quita intercepto
Xa <- sparse.model.matrix(~.+0, data = Xa)

#vector de Y´s
Ya<-data_training_a$churn

#CV LASSO
cvlasso_a<-cv.gamlr(x = Xa, y = Ya, verb = T, family = 'binomial', nfold = 5)

#Grafica
plot(cvlasso_a)
```


### 5. Grafica el Lasso de los coeficientes vs la complejidad del modelo.
```{r}
#plot(cv_lasso$gamlr, select=FALSE)
plot(cvlasso_a$gamlr)
```


\newpage

### 6 (2 pts). Cuál es la $\lambda$ resultante? Genera una tabla con los coeficientes que selecciona el CV LASSO. Cuántas variables deja iguales a cero? Cuales son las 3 variables más importantes para predecir el abandono? Da una explicación intuitiva a la última pregunta
```{r}
#coef(cv_lasso, select="min") # Coeficientes de CV LASSO
coef(cvlasso_a, select="min", k=2, corrected=TRUE) #a
```


```{r}
#lambda_id <- colnames(coef(cv_lasso, select="min")) # Identificador para el lambda deseado
#cv_lasso$gamlr$lambda[lambda_id] # Valor del lambda deseado


#lambda resultante
#A
a_lambda<- colnames(coef(cvlasso_a, select="min"))
cvlasso_a$gamlr$lambda[a_lambda]

```

### 7. Genera un data frame (usando el validation set) que tenga: `customer`, `churn` y las predicciones del LASSO.
```{r}

#Predicciones
#y_pred<-predict(cv_lasso$gamlr,
#                newdata = data_validation[,-1],
#                type = 'response',
#                select = cv_lasso$seg.min)
#y_pred<-as.numeric(y_pred)

#A
lasso_score <- predict(cvlasso_a,
               newdata = data_validation[,-1],
               type='response',
               select = "min")

# FVG
#lasso_predict <- as.numeric(lasso_score > 0.5)

#dataframe
A <- data.frame(data_validation$customer, churn_validation, lasso_score, lasso_predict)
colnames(A)[3:4] <- c('lasso_score', 'lasso_predict')

```

### 8. Estima ahora tree. Usa `mindev = 0.05, mincut = 1000` Cuántos nodos terminales salen? Muestra el summary del árbol
```{r}
#A
tree_control <- tree.control(nobs=nrow(data_training_a[,-1]),
             mincut = 1000,
             mindev = 0.05)


tree_estimation <- tree(as.factor(churn) ~ . ,
                        data = data_training_a[,-1],
                        control = tree_control,
                        split = c("gini"))

summary(tree_estimation)

```
Obtenemos `r summary(tree_estimation)$size ` nodos finales

### 9. Grafica el árbol resultante
```{r}
plot(tree_estimation)
text(tree_estimation,pretty=0)
title("Árbol para predicción de churn",line=0.5)

```

### 10. Poda el árbol usando CV. Muestra el resultado. Grafica Tree Size vs Binomial Deviance. Cuál es el mejor tamaño del árbol? Mejora el Error?
```{r}
cv_tree <- cv.tree(tree_estimation, K=10)
cv_tree


# Size con menor deviance
min_dev_size <- cv_tree$size[match(min(cv_tree$dev),cv_tree$dev)]


# Gráfica Tree Size vs. Binomial Deviance
plot_size_dev <- function(cv_tree){

  ggplot(data=as.data.frame(cbind(size=cv_tree$size,dev=cv_tree$dev)),
       aes(x=size,y=dev))+
  geom_point(size=3)+
  labs(title="Deviance against Tree Size")+
  xlab("Tree Size")+
  ylab("Deviance")+
  theme_bw()+
    theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

                                  }

plot_size_dev(cv_tree)
```
Por lo general buscaríamos el árbol que nos de menor _deviance_ , en este caso tenemos una _deviance_ constante y elegimos el modelo de 25 nodos. Esto nos podría indicar que los split son espurios y probablemente este modelo no sea el adecuado para llevar a cabo la predicción de _churn_

### 11. Gráfica el árbol final. (Tip: Checa `prune.tree`)
```{r}
tree_cut <- prune.tree(tree_estimation, best = min_dev_size)


plot(tree_cut)
text(tree_cut,pretty=0)
title("Árbol final para predicción de churn",line=0.5)

```

### 12. Genera las predicciones del árbol pruned. Guardalas en la base de predicciones. Guarda el score y la prediccion categorica en la misma data frame donde guardaste las predicciones del LASSO
```{r}
tree_score_predict <- predict(tree_cut,newdata=data_validation)

tree_class_predict <- predict(tree_cut,newdata=data_validation,type="class")

# unir con base de predicciones
A <- data.frame(A, tree_score_predict[,2], tree_class_predict)
names(A)[5:6] <- c("tree_score","tree_predict")
```


### 13 (4pts). Corre un Random Forest ahora. Cuál es la $B$ para la que ya no ganamos mucho más en poder predictivo?

- Corre para `num.trees=100,200,300, 500, 700, 800`

- En cada caso, guarda únicamente el `prediction.error`
```{r}
# eficientar el Random Forest corriendolo en los nodos disponibles del equipo
cl<- detectCores() %>% makeCluster()
cl

# vector con el número de los arboles
trees <- c()
# vector con el prediction error de cada forest
error <- c()

# Estimation. Loop que estima un random forest para distintos numeros de árboles

a<-Sys.time() # para observar el tiempo de estimacion

k <- 1 # iterador
for (i in c(100,200,300,500,700,800)){

  rf<-ranger(churn~.,
             data = data_training_a[-1],
             num.trees = i,
             mtry = (ncol(data_training_a)-2) %>% sqrt() %>% floor(),
             min.node.size = 1,
             splitrule = "gini",
             classification = T,
             )

  trees[k] <- rf$num.trees
  error[k] <- rf$prediction.error

  rf <- NULL
  k <- k + 1
}

Sys.time() -a
stopCluster(cl)

rf <- data.frame(trees,error)
rf


```
Parece ser que con una $B=200$, el error de predicción ya no parece reducirse mucho. Cabe mencionar que el error in sample era mucho menor con las bases resultantes de oversampling, pero lo opuesto pasaba en OOS.

### 14. Escoge un random forest para hacer las predicciones. Grafica la importancia de las variables. Interpreta

```{r}
# vuelvo a correr el mejor random forest
cl<- detectCores() %>% makeCluster()
cl
best_rf <- ranger(churn~.,
             data = data_training_a[,-1],
             num.trees = 200,
             mtry = (ncol(data_training_a)-2) %>% sqrt() %>% floor(),
             importance = "impurity",
             classification = T)

stopCluster(cl)

# Grafica
df_imp <- data.frame(names=(importance(best_rf) %>%
              names()),importance=(importance(best_rf)))


ggplot(df_imp,aes(x=reorder(names,importance),y=importance)) +
  geom_bar(stat="identity") +
  xlab("Variable")+
  ylab("importancia")+
  coord_flip()

```


### 15. Genera las predicciones OOS para el random forest. Guardalas en la misma data.frame que los otros modelos
```{r}
pred_rf<-predict(best_rf, data = (data_validation[,-1]), type = "response")

# añadir prediccion al data frame con las otras predicciones

A <- data.frame(A,pred_rf$predictions)
names(A)[7] <- "rf_predict"
```


### 16 (2pts). Corre el mismo forest pero ahora con `probability = T`. Esto generará predicciones númericas en lugar de categóricas. Genera las predicciones continuas y guardalas en el mismo data frame
```{r}
# vuelvo a correr el mejor random forest, con probability=T
cl<- detectCores() %>% makeCluster()
cl
best_rf2 <- ranger(churn~.,
             data = data_training_a[,-1],
             num.trees = 200,
             mtry = (ncol(data_training_a)-2) %>% sqrt() %>% floor(),
             importance = "impurity",
             classification = T,
             probability = T
             )
stopCluster(cl)

# prediccion continua de churn==1
# eval$pred_rf_cont <- predict(best_rf2, data=data_validation)$predictions[,1]
rf_score <- predict(best_rf2, data=data_validation[,-1])

A <- data.frame(A, rf_score$predictions[,1])
names(A)[8] <- "rf_score"
A <- A[c(1,2,3,4,5,6,8,7)]
```

### 17 (4 pts). Genera graficas de las curvas ROC para los tres modelos. Cual parece ser mejor?
```{r}
library(yardstick)
summary(A)

# Lasso
roc_curve(data = A,
          truth = as.factor(churn_validation),
          lasso_score,
          event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +
  ggtitle("Curva ROC del LASSO")+
  geom_path() +
  theme_bw()

# Trees
roc_curve(data = A,
           truth = as.factor(churn_validation),
           tree_score,
           event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity))+
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed')+
  ggtitle("Curva ROC del Prune Tree")+
  geom_path()+
  theme_bw()

# Random Forest
roc_curve(data = A, 
          truth = factor(churn_validation),
          rf_score,
          event_level = "second") %>%
  ggplot(aes(x = 1-specificity, y = sensitivity)) +
  geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +
  ggtitle("Curva ROC del Random Forest")+
  geom_path() +
  theme_bw()

```

Dado que la línea punteada es un modelo nulo (aleatorio), entre más rápido gane sensibilidad sin perder especificidad, el modelo será mejor. Gráficamente, buscamos una curva cóncava y pegada al eje y del lado derecho y al eje x por arriba. En este sentido, el Random Forest parece ser el mejor modelo.

### 18. Genera una tabla con el AUC ROC. Cuál es el mejor modelo ?
```{r}
# roc_auc(eval, factor(validation), pred_rf, event_level = "second")
lasso_auc <- roc_auc(data = A, 
                     truth = factor(churn_validation),
                     lasso_score, 
                     event_level = "second")

tree_auc <- roc_auc(data = A, 
                    truth = factor(churn_validation),
                    tree_score, 
                    event_level = "second")

rf_auc <- roc_auc(data = A, 
                  truth = factor(churn_validation),
                  rf_score, 
                  event_level = "second")

aucs <- (full_join(lasso_auc, tree_auc) %>% 
  full_join(rf_auc))[,3] %>% 
  rename(AUC = .estimate) %>% as.data.frame()
rownames(aucs) <- c('CV-LASSO','Pruned Tree','Random Forest')

kable(aucs)


```

Confirmamos que el mejor modelo es el Random Forest.

### 19 (2pts). Escoge un punto de corte para generar predicciones categoricas para el LASSO basado en la Curva ROC. Genera las matrices de confusión para cada modelo. Compáralas. Qué tipo de error es mas pernicioso?
```{r}
# ------------------------------
# Puntos de corte
# ------------------------------
# Nota (FVG): Ya hice el punto de corte para LASSO desde que 
#             se definió inicialmente el df A en la sección de LASSO.
#             Lo hice en 0.5, y me parece que el resto de los modelos toman el mismo criterio.

summary(A$lasso_score)
summary(A$lasso_predict)

# Loop que tome diferentes puntos de corte y grafique curva ROC sobre el predict

gglist <- NULL
it <- 1 # iterador

for (x in seq(0.3,0.8,0.1)){

  A$lasso_1 <- as.numeric(A$lasso_score > x)
  
  curva <- roc_curve(data = A,
          truth = as.factor(churn_validation),
          lasso_1,
          event_level = "second") %>%
          ggplot(aes(x = 1-specificity, y = sensitivity)) +
          geom_abline(slope = 1, intercept = 0, linetype = 'dashed') +
          ggtitle("Curva ROC del LASSO")+
          geom_path() +
          theme_bw()
  
  gglist[[it]] <- curva
  
  curva <- NULL
  it <- it + 1
}

grid.arrange(grobs=gglist,ncol=4)
```
Se puede ver que el mejor punto de corte es cercano al 0.5. 

PENDIENTE: si les late esta forma de responder, sigo iterando para encontrar un óptimo.


```{r}
# ------------------------------
# Matrices de confusión
# ------------------------------

# LASSO
matconf_lasso <- caret::confusionMatrix(as.factor(A$lasso_predict),as.factor(A$churn_validation))

# TREE
matconf_tree <- caret::confusionMatrix(as.factor(A$tree_predict),as.factor(A$churn_validation))

# RANDOM FOREST
matconf_rf<-caret::confusionMatrix(as.factor(A$rf_predict),as.factor(A$churn_validation))

matconf_lasso$table
matconf_tree$table
matconf_rf$table

```
Como vimos en clase, a partir de las matrices de confusión es posible calcular 5 métricas de éxito:
- Sensibilidad
- Especificidad
- Positivos Acertados
- Negativos Acertados
- Accuracy

A continuación se comparan dichas métricas en una tabla:

```{r}

exito<- as.data.frame(rbind(matconf_lasso$byClass[1:5],
      matconf_tree$byClass[1:5],
      matconf_rf$byClass[1:5]))

Modelo <- c("Lasso","Tree","Random Forest")
predictive_power <- cbind(Modelo,exito)
rm(exito, Modelo)

kable(predictive_power,booktabs=T, align = 'c',
      col.names = c("Modelo","Sensibilidad", "Especificidad","Positivos acertados","Negativos acertados", "Accuracy"),digits = 3)%>%
  kable_styling(position = "center",
                latex_options="HOLD_position")

```
El modelo que arroja las mejores métricas de éxito es el _Random Forest_. 
En este caso nos interesa una sensibilidad relativamente alta, ya que nos indica cuál es la probabilidad de que encontremos a clientes que efectivamente se van ($churn=1$).

Adicionalmente, nos interesa ver el porcentaje de positivos acertados, lo que nos dice cuántos clientes que predecimos que se vayan efectivamente se irán.



### 20 (2pts). Finalmente, construye una lift table. Esto es, para 20 grupos del score predecido, genera 1) El promedio de las predicciones, 2) el promedio del churn observado. Existe monotonía? El mejor algoritmo es monotónico? (Tip: usa `ntile` para generar los grupos a partir de las predicciones)
```{r}
# tablas
tabla_lasso<-liftTable( A$lasso_score, A$churn_validation, resolution = 1/20)
tabla_tree<-liftTable( A$tree_score, A$churn_validation, resolution = 1/20)
tabla_rf<-liftTable( A$rf_score, A$churn_validation, resolution = 1/20)
tabla_lift<-data.frame(tabla_lasso$Percentile,tabla_lasso$expectedIncidence,
                       tabla_lasso$trueIncidence,tabla_tree$trueIncidence,tabla_rf$trueIncidence)
kable(tabla_lift, col.names = c("Percentil","Valor esperado","Promedio Lasso","Promedio Tree","Promedio RF"))



# Esto es para generar las graficas
tabla_lift <- function(variable_objetivo, score_prediccion, groups) {
if(is.factor(variable_objetivo)) variable_objetivo <- as.integer(as.character(variable_objetivo))
if(is.factor(score_prediccion)) score_prediccion <- as.integer(as.character(score_prediccion))
helper = data.frame(cbind(variable_objetivo, score_prediccion))
helper[,"bucket"] = ntile(-helper[,"score_prediccion"], groups)
gaintable = helper %>% group_by(bucket)  %>%
  summarise_at(vars(variable_objetivo), funs(total = n(),
  totalresp=sum(., na.rm = TRUE))) %>%
  mutate(Cumresp = cumsum(totalresp),
  Gain=Cumresp/sum(totalresp)*100,
  Cumlift=Gain/(bucket*(100/groups)))
return(gaintable)
}

#El tree es un desmadre JAJA
lift_lasso<-tabla_lift(A$churn_validation , A$lasso_score, groups = 20)
graphics::plot(lift_lasso$bucket, lift_lasso$Cumlift, type="l", ylab="Lift acumulado", xlab="Cubeta",main = "Lasso")
lift_tree<-tabla_lift(A$churn_validation , A$tree_score, groups = 20)
graphics::plot(lift_tree$bucket, lift_tree$Cumlift, type="l", ylab="Lift acumulado", xlab="Cubeta",main = "Tree")
lift_rf<-tabla_lift(A$churn_validation , A$rf_score, groups = 20)
graphics::plot(lift_rf$bucket, lift_rf$Cumlift, type="l", ylab="Lift acumulado", xlab="Cubeta",main = "Random Forest")

```


### 21. Concluye. Que estrategia harías con este modelo? Cómo generarías valor a partir de el?
```{r}


```

### EXTRA: XGB

```{r}
# Preparar la base de entrenamiento
sparse_train <- sparse.model.matrix(~.+0, data = data_training_a[,-c(1,2)]) 
label_train <- data_training_a[,2]
dtrain <- xgb.DMatrix(sparse_train, label = label_train) # Label es el target

# Preparar la base de validación
data_validationn <- data[-training_rows,]
sparse_test <- sparse.model.matrix(~.+0, data = data_validation[,-c(1)]) 
label_test <- data_validationn[,2]
dtest <- xgb.DMatrix(sparse_test, label = label_test)
watchlist <- list(train = dtrain, eval = dtest) # Para evaluar el performance del modelo

# Entrenamiento del modelo
param <- list(max_depth = 6, learning_rate = 0.06, objective = "binary:logistic", 
              eval_metric = "auc", subsample = 0.85, colsample_bytree = 0.7)
bst <- xgb.train(params = param, dtrain, early_stopping_rounds =  10, nrounds = 100, 
                 watchlist)
# Actualmente le pego al 0.676 con 6, 0.06, 0.85, 0.7, 10, 100

# Predicción
pred <- predict(bst, sparse_test)

```

```{r}
# Plot: Grafica todos los trees. 
# NOTA: Muy pesado y poco visible a menos que nround sea bajo.
# xgb.plot.tree(model = bst)
```


