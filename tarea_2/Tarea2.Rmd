---
output:
  pdf_document: null
  geometry: margin=1in
  html_document:
    df_print: paged
fontsize: 11pt
header-includes: \usepackage{geometry} \usepackage{graphicx} \tolerance=1 \hyphenpenalty=10000
  \hbadness=10000 \linespread{1.2} \usepackage[justification=centering, font=bf, labelsep=period,
  skip=5pt]{caption} \usepackage{titling} \usepackage{babel} \usepackage{fancyhdr}
  \pagestyle{fancy} \fancyhead[L]{Maestría en Economía Aplicada} \fancyhead[R]{ITAM}
---
\begin{titlepage}
\begin{center}

\textsc{\Large Instituto Tecnológico Autónomo de México}\\[2em]

\textbf{\LARGE Economía Computacional}\\[2em]


\textsc{\LARGE }\\[1em]


\textsc{\LARGE Tarea 2 }\\[1em]
\textsc{\large }\\[1em]
\textsc{\LARGE Equipo 7 }\\[1em]


\textsc{\large }\\[1em]
\textsc{\LARGE }\\[1em]


\textsc{\large }\\[1em]
\textsc{\LARGE }\\[1em]

\textsc{\LARGE Prof. Isidoro García Urquieta}\\[1em]

\textsc{\LARGE }\\[1em]
\textsc{\LARGE }\\[1em]

\textsc{\LARGE Alfredo Lefranc Flores}\\[1em]

\textsc{\large 144346}\\[1em]

\textsc{\LARGE Cynthia Raquel Valdivia Tirado }\\[1em]

\textsc{\large 81358}\\[1em]

\textsc{\LARGE Rafael Sandoval Fernández}\\[1em]

\textsc{\large 143689}\\[1em]

\textsc{\LARGE Marco Antonio Ramos Juárez}\\[1em]

\textsc{\large 142244}\\[1em]

\textsc{\LARGE Francisco Velazquez Guadarrama}\\[1em]

\textsc{\large 175606}\\[1em]

\end{center}

\vspace*{\fill}
\textsc{Ciudad de México \hspace*{\fill} 2021}

\end{titlepage}


\newpage


\tableofcontents

\newpage

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, message = FALSE,
                      fig.width = 7, fig.height = 4, fig.align = "right")
```

```{r,message=FALSE }
packages <- c(
  "tidyverse",
  "data.table",
  'broom',
  'knitr',
  'lubridate',
  'RCT',
  'gamlr',
  'ranger',
  'tree',
  'parallel',
  'naniar',
  'kableExtra',
  'gridExtra',
  'VIM',
  'smotefamily'
  )

# instala los que no tengas
installed_packages <- packages %in% rownames(installed.packages())
if (any(installed_packages == FALSE)) {
  install.packages(packages[!installed_packages])
}

# cargar paquetes
lapply(packages,
       library,
       character.only = TRUE)

# funciones

check_nas <- function(df){
  df %>%
    select_if(~sum(is.na(.)) > 0) %>%
    miss_var_summary()
}

```



## Contexto

Cell2Cell es una compañía de teléfonos celulares que intenta mitigar el abandono de sus usuarios. Te contratan para 1) Encontrar un modelo que prediga el abandono con acierto y para usar los insights de este modelo para proponer una estrategia de manejo de abandono.


Las preguntas que contestaremos son:

1. Se puede predecir el abandono con los datos que nos compartieron?

2. Cuáles son las variables que explican en mayor medida el abandono?

3. Qué incentivos da Cell2Cell a sus usarios para prevenir el abandono?

4. Cuál es el valor de una estrategia de prevención de abandono focalizada y cómo difiere entre los segmentos de los usuarios? Qué usuarios deberían de recibir incentivos de prevención? Qué montos de incentivos

Nota: Voy a evaluar las tareas con base en la respuesta a cada pregunta. Como hay algunas preguntas que no tienen una respuesta clara, al final ponderaré de acuerdo al poder predictivo de su modelo vs las respuestas sugeridas.



\newpage

## Datos

Los datos los pueden encontrar en `Cell2Cell.Rdata`. En el archivo `Cell2Cell-Database-Documentation.xlsx` pueden encontrar documentación de la base de datos.

Carguemos los datos
```{r }
load('Cell2Cell.Rdata') %>% as.data.frame()
# file path de archivo
# path_data <- file.path('C:/Users/rsf94/Documents/economia_computacional/tarea_2',
#                       'Cell2Cell.RData')

# renombrar como data
data <- as.data.frame(cell2cell)
rm(cell2cell)
```

### 1. Qué variables tienen missing values? Toma alguna decisión con los missing values. Justifica tu respuesta
```{r,results='asis'}
# Rafa
# tabla resumen de missing values
kable(check_nas(data),booktabs=T, align = 'c',
      col.names = c("Variale", "Cantidad","%"),digits = 2)%>%
  kable_styling(position = "center",
                latex_options="HOLD_position")
```

```{r}
# columnas con NAs
cols_con_nas <- names(which(colSums(is.na(data)) > 0))

df_nas <- data %>% select(all_of(cols_con_nas)) %>% as.data.frame()
summary(df_nas)
```

Lista de las variables con valores faltantes. Todas son numéricas. Podríamos hacer imputación.


```{r}

# numero de filas con NAs
data[!complete.cases(data),] %>% nrow()

# correlaciones entre variables con NAs
x <- as.data.frame(abs(is.na(data)))
# Extrae las variables que tienen algunas celdas con NAs
y <- x[which(sapply(x, sd) > 0)]
# la sd va a ser positiva para variables que tengan NAs

# Da la correación un valor alto positivo significa que desaparecen juntas.
cor(y)

# Visualización de los NAs
#aggr(data, plot=FALSE, prop=FALSE, numbers=TRUE)

```


Las 216 valores faltantes para revenue, mou, recchrge, directas, overage y roam coinciden.Lo mismo pasa con los 502 valores faltantes de changem y changer, con el valor faltante de phones, models, eqpdays, y con los 1244 valores faltantes de age1 y age2. Además, se puede ver en la matriz que los valores faltantes del primer grupo de variables coincide en un 65% con los valores faltantes de changem y changer. Para estos casos, se tiene valores faltantes en 8 de 68 variables, el 11.7% de las columnas.

Esto en general nos dice que no hay un problema serio de NAs. No obstante, para no perder dicha información, lo mejor sería hacer imputación.

Podemos imputar la media a aquellas variables que parezcan tener una distribución normal.

```{r}
myhist <- function(yvar){
  ggplot(df_nas, aes_(x=as.name(yvar)))+
    geom_histogram()+
    ggtitle(paste0(as.name(yvar)))+
    xlab("")+
    ylab("")+
    theme(axis.text.y = element_blank())
}
hists<- df_nas %>%
        names() %>%
        lapply(myhist)


grid.arrange(grobs=hists,ncol=4)
```

Parece que las variables mou, recchrge, eqpdays se distribuyen de forma normal.


```{r}
# se define una funcion para imputar por valor central
imputar_valor_central <- function(data, var_list) {
  # Función para imputar valores centrales, media/mediana en numéricos y moda en categoricos
  # Inputs:
  # data - conjunto de datos
  # colnames - array de las columnas que se desea imputar
  #
  # Outputs
  # dataframe con imputaciones centrales

  if (missing(var_list)){
    var_list <- colnames(data)
  }

  #Dividir entre numericas y categoricas
  data_columnas <- data %>%
    dplyr::select(all_of(var_list)) %>% as.data.frame()
  var_numericas <- dplyr::select_if(data_columnas, is.numeric) %>% names()
  var_categoricas <- dplyr::select_if(data_columnas, is.character) %>% colnames()

  #Imputar
  algas_data_imputacion_central  <-
    data.frame(data) %>%
    # variables numéricas (media)
    mutate_at(
      vars(var_numericas),
      funs(ifelse(is.na(.), median(., na.rm = T), .))) %>%
    # variables categóricas (moda)
    mutate_at(
      vars(var_categoricas),
      funs(as.ordered(ifelse(is.na(.), moda(.), as.character(.)))))

  return(algas_data_imputacion_central)


}

# se aplica la funcion al conjunto de datos
df_nas2 <- imputar_valor_central(df_nas)

summary(df_nas2)

```



### 2. Tabula la distribución de la variable `churn`. Muestra la frecuencia absoluta y relativa. Crees que se debe hacer oversampling/undersamping?
```{r, out.width = '70%',fig.align = "center"}

# tabulación
tabulado <- data %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
  mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta))

kable(tabulado,booktabs=T, align = 'c',
      col.names = c("Churn", "Cantidad","%"),digits = 2)%>%
  kable_styling(position = "center",
                latex_options="HOLD_position")

rm(tabulado)


data$churn <- as.numeric(data$churn)
# frecuencia absoluta
ggplot(data, aes(x=churn))+
  geom_bar(aes(y=..count..,fill=(factor(..x..)),stat="count"))+
  geom_text(aes(label= scales::number(..count..),
                y=..count..),stat="count", vjust=-0.5)+
  labs(title="Distribución de churn",
    subtitle="Frecuencia absoluta",
    fill="Churn")+
  ylab("Frecuencia absoluta")+
  scale_y_continuous(labels=scales::comma,
                     limits=c(0,60000))+
  scale_x_continuous(breaks=c(0,1))+
  theme_bw()+
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

# frecuencia relativa
ggplot(data, aes(x=churn))+
  geom_bar(aes(y=..prop.., fill=factor(..x..)),stat="count")+
    geom_text(aes(label= scales::percent(..prop..),
                y=..prop..),stat="count", vjust=-0.5)+
  scale_y_continuous(breaks=seq(0,1, by =0.1),
                     limits=c(0,1))+
  scale_x_continuous(breaks=c(0,1))+

    labs(title="Distribución de churn",
       subtitle="Frecuencia relativa",
       fill="Churn")+
  ylab("Frecuencia relativa")+
  theme_bw()+
  theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

```

Sí, parece que sí se debe hacer algún tipo de remuestreo para tener una base balanceada y que los modelos a estimar tengan mayor poder predictivo.


### 3. (2 pts) Divide tu base en entrenamiento y validación (80/20). Además, considera hacer oversampling (SMOTE) o undersampling. (Tip: Recuerda que el objetivo final es tener muestra ~balanceada en el traning set. En el validation la distribución debe ser la original)

Primero dividimos la base.
```{r}
set.seed(123)

#PARCHE TEMPORAL
#data[is.na(data)] <- 0 # Parche temporal, vuelve 0 los NA


# proporción que queremos de training
training_size <- 0.8

# filas de training
training_rows <- sample(seq_len(nrow(data)),size=floor(training_size*nrow(data)))

#training set
data_training <- data[training_rows,]

#validation set
data_validation <- data[-training_rows,]

# la muestra está balanceada? --> NO :(
(tabulado <- data_training %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
  mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))
```
No está balanceada, aunque sí conserva la proporción de clases de la base original. Procedemos a rebalancear, tenemos 3 estrategias: undersampling, oversampling o una mezcla de ambas.

Cabe mencionar que realizamos el rebalanceo sobre nuestra base de entrenamiento.

```{r undersampling}

# 1. Undersampling

# tamaño de churn==1 en la base de entrenamiento
freq_churn <- data_training %>% filter(churn==1) %>% nrow()


#simplemente reducimos la clase más abundante
undersampling_c0<-  sample_n((data_training %>% filter(churn==0)),
                             size=freq_churn,
                             replace = FALSE)

undersampling<-rbind((data_training %>%
                        filter(churn==1)),undersampling_c0)



```

```{r oversampling}
# 2. oversampling
# Segundo intento, sugiere hacer Synthetic Minority Oversampling Technique (SMOTE) librería smotefamily
# echénle un ojo a: https://rikunert.com/SMOTE_explained
smote <- smotefamily::SMOTE((data_training %>% na.exclude)[,-2], # data
                            (data_training %>% na.exclude)$churn, # class attribute
                            K=10, # number of nearest neighbors
                            dup_size=2 # desired times of synthetic minority instances
                            # over original number of majority instances
                            )

class(data %>% na.exclude)
oversampled <- as.data.frame(smote$data)

# smote guarda la variable de clase como class. se renombra y convierte a numerica
colnames(oversampled)[68] <- "churn"
oversampled$churn <- oversampled$churn %>% as.numeric

# sí está balanceado, aún no entiendo bien los outputs de la función
(tabulado <- oversampled %>% group_by(churn) %>%
    dplyr::summarise(frecuencia_absoluta=n()) %>%
    mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))


# $data arroja el data frame original + observaciones sintéticas de la minoría
length(as.data.frame(smote$data)$churn)

# syn_data arroja observaciones sintéticas de la minoría
length(as.data.frame(smote$syn_data)$churn)

# resta de ambos nos da el original
length((data_training %>% na.exclude)$churn)


```


```{r under-oversampling}

#Under y over sampling
#para rebalancear de manera más precisa el oversampling, simplemente "podamos" las observaciones sinteticas de tal manera que nos quede una base de datos perfectamente balanceada.

freq_churn_os <- oversampled %>% filter(churn==0) %>% nrow()

#podamos
undersampling_c1<-  sample_n((oversampled %>% filter(churn==1)),
                             size=freq_churn_os,replace = FALSE)

#armamos la base
under_over_sampling<-rbind((oversampled %>% filter(churn==0)),undersampling_c1)

#listo!
(tabulado <- under_over_sampling %>% group_by(churn) %>% dplyr::summarise(frecuencia_absoluta=n()) %>%
    mutate(frecuencia_relativa = frecuencia_absoluta/sum(frecuencia_absoluta)))
```


De esta manera me quedan 3 bases para comparar: undersampling, oversampling y una combinación de ambos.

```{r under-oversampling}
data_a<-undersampling
data_b<-oversampled
data_c<-under_over_sampling

str(data_a)
str(data_b)
str(data_c)

```


Ahora experimentaremos con cada base:


```{r SMOTE CV}

training_size <- 0.8

# A
training_rows_a <- sample(seq_len(nrow(data_a)),size=floor(training_size*nrow(data_a)))
data_training_a <- data_a[training_rows,] %>% na.exclude()
data_validation_a <- data_a[-training_rows,]

# B
training_rows_b <- sample(seq_len(nrow(data_b)),size=floor(training_size*nrow(data_b)))
data_training_b <- data_b[training_rows,]
data_validation_b <- data_b[-training_rows,]

# C
training_rows_c <- sample(seq_len(nrow(data_c)),size=floor(training_size*nrow(data_c)))
data_training_c <- data_c[training_rows,]
data_validation_c <- data_c[-training_rows,]

data_training_a[]

```






## Model estimation

Pondremos a competir 3 modelos:

1. Cross-Validated LASSO-logit

2. Prune Trees

3. Random Forest

### 4 (2 pts). Estima un cross validated LASSO. Muestra el la gráfica de CV Binomial Deviance vs Complejidad
```{r}
data_training[is.na(data_training)] <- 0 # Parche temporal, vuelve 0 los NA

X <- sparse.model.matrix(~.+0, data = data_training[,-c(1,2)]) # Transformar a sparse matrix la información relevante

cv_lasso <- cv.gamlr(x = X, y = data_training$churn, family = 'binomial', nfold = 5, verb = TRUE) # Estimar el CV LASSO

par(mar=c(5,4,4,2) + 0.1)
plot(cv_lasso) # Gráfico

```

### 5. Grafica el Lasso de los coeficientes vs la complejidad del modelo.
```{r}
plot(cv_lasso$gamlr, select=FALSE)
```

\newpage

### 6 (2 pts). Cuál es la $\lambda$ resultante? Genera una tabla con los coeficientes que selecciona el CV LASSO. Cuántas variables deja iguales a cero? Cuales son las 3 variables más importantes para predecir el abandono? Da una explicación intuitiva a la última pregunta
```{r}
coef(cv_lasso, select="min") # Coeficientes de CV LASSO
```

```{r}
lambda_id <- colnames(coef(cv_lasso, select="min")) # Identificador para el lambda deseado
cv_lasso$gamlr$lambda[lambda_id] # Valor del lambda deseado
```

### 7. Genera un data frame (usando el validation set) que tenga: `customer`, `churn` y las predicciones del LASSO.
```{r}

```


### 8. Estima ahora tree. Usa `mindev = 0.05, mincut = 1000` Cuántos nodos terminales salen? Muestra el summary del árbol
```{r}
# hago para los 3 training datasets que tenemos: a,b y c
# b2 es restringiendo variables

#A
tree_control_a <- tree.control(nobs=nrow(data_training_a),
             mincut = 1000,
             mindev = 0.05)


tree_estimation_a <- tree(as.factor(churn) ~ . -customer,
                        data = data_training_a,
                        control = tree_control_a,
                        split = c("gini"))

#B
tree_control_b <- tree.control(nobs=nrow(data_training_b),
             mincut = 1000,
             mindev = 0.05)


tree_estimation_b <- tree(as.factor(churn) ~ . -customer ,
                        data = data_training_b,
                        control = tree_control_b)


tree_estimation_b2 <- tree(as.factor(churn) ~ eqpdays + refurb + changem + months + mou + changer + recchrge + creditcd + setprc + revenue + retcall + marryyes + credita,
                        data = data_training_b)

#C
tree_control_c <- tree.control(nobs=nrow(data_training_c),
             mincut = 1000,
             mindev = 0.05)


tree_estimation_c <- tree(as.factor(churn) ~ . -customer,
                        data = data_training_c,
                        control = tree_control_c)



summary(tree_estimation_a)

summary(tree_estimation_b)
summary(tree_estimation_b2)
summary(tree_estimation_c)

# El a utiliza más variables

```

### 9. Grafica el árbol resultante
```{r}
# me gusta el a

plot(tree_estimation_a)
text(tree_estimation_a,pretty=0)

plot(tree_estimation_b)
text(tree_estimation_b,pretty=0)

plot(tree_estimation_b2)
text(tree_estimation_b2,pretty=0)

plot(tree_estimation_c)
text(tree_estimation_c,pretty=0)


#cross validation y confusionMatrix (opcional)
#data_validation_a$churn <- as.factor(data_validation_a$churn)

#DTpred<- predict(tree_estimation_a,type="class" ,newdata=data_validation_a)
#caret::confusionMatrix(data_validation_a$churn,DTpred)

```

### 10. Poda el árbol usando CV. Muestra el resultado. Grafica Tree Size vs Binomial Deviance. Cuál es el mejor tamaño del árbol? Mejora el Error?
```{r}
cv_tree_a <- cv.tree(tree_estimation_a, K=10)
cv_tree_b <- cv.tree(tree_estimation_b, K=10)
cv_tree_b2 <- cv.tree(tree_estimation_b2, K=10)
cv_tree_c <- cv.tree(tree_estimation_c, K=10)

cv_tree_a
cv_tree_b
cv_tree_b2
cv_tree_c

# Size con menor deviance
min_dev_size_a <- cv_tree_a$size[match(min(cv_tree_a$dev),cv_tree_a$dev)]
min_dev_size_b <-cv_tree_b$size[match(min(cv_tree_b$dev),cv_tree_b$dev)]
min_dev_size_b2 <-cv_tree_b2$size[match(min(cv_tree_b2$dev),cv_tree_b2$dev)]
min_dev_size_c <-cv_tree_c$size[match(min(cv_tree_c$dev),cv_tree_c$dev)]

# Gráfica Tree Size vs. Binomial Deviance

plot_size_dev <- function(cv_tree){

  ggplot(data=as.data.frame(cbind(size=cv_tree$size,dev=cv_tree$dev)),
       aes(x=size,y=dev))+
  geom_point(size=3)+
  labs(title="Deviance against Tree Size")+
  xlab("Tree Size")+
  ylab("Deviance")+
  theme_bw()+
    theme(axis.line = element_line(colour = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.background = element_blank())

                                  }

plot_size_dev(cv_tree_a)
plot_size_dev(cv_tree_b)
plot_size_dev(cv_tree_b2)
plot_size_dev(cv_tree_c)


```

### 11. Gráfica el árbol final. (Tip: Checa `prune.tree`)
```{r}
tree_cut_a <- prune.tree(tree_estimation_a, best = min_dev_size_a)
tree_cut_b <- prune.tree(tree_estimation_b, best = min_dev_size_b)
tree_cut_b2 <- prune.tree(tree_estimation_b2, best = min_dev_size_b2)
tree_cut_c <- prune.tree(tree_estimation_c, best = min_dev_size_c)

plot(tree_cut_a)
text(tree_cut_a,pretty=0)

plot(tree_cut_b)
text(tree_cut_b,pretty=0)

plot(tree_cut_b2)
text(tree_cut_b2,pretty=0)

plot(tree_cut_c)
text(tree_cut_c,pretty=0)

```

### 12. Genera las predicciones del árbol pruned. Guardalas en la base de predicciones. Guarda el score y la prediccion categorica en la misma data frame donde guardaste las predicciones del LASSO
```{r}

tree_score_predict_a <- predict(tree_cut_a,data=data_validation_a)
tree_class_predict_a <- predict(tree_cut_a,data=data_validation_a,type="class")

tree_score_predict_b <- predict(tree_cut_b,data=data_validation_b)
tree_class_predict_b <- predict(tree_cut_b,data=data_validation_b,type="class")

tree_score_predict_b2 <- predict(tree_cut_b2,data=data_validation_b2)
tree_class_predict_b2 <- predict(tree_cut_b2,data=data_validation_b2,type="class")

tree_score_predict_c <- predict(tree_cut_c,data=data_validation_c)
tree_class_predict_c <- predict(tree_cut_c,data=data_validation_c,type="class")


# unir con base de predicciones


```


### 13 (4pts). Corre un Random Forest ahora. Cuál es la $B$ para la que ya no ganamos mucho más en poder predictivo?

- Corre para `num.trees=100,200,300, 500, 700, 800`

- En cada caso, guarda únicamente el `prediction.error`
```{r}
# eficientar el Random Forest corriendolo en los nodos disponibles del equipo
cl<- detectCores() %>% makeCluster()
cl

# vector con el número de los arboles
trees <- c()
# vector con el prediction error de cada forest
error <- c()

# Estimation. Loop que estima un random forest para distintos numeros de árboles

a<-Sys.time() # para observar el tiempo de estimacion

k <- 1 # iterador
for (i in c(100,200,300,500,700,800)){

  rf<-ranger(churn~.,
             data = data_training_a,
             num.trees = i,
             mtry = (ncol(data_training_a)-1) %>% sqrt() %>% floor(),
             min.node.size = 1,
             splitrule = "gini",
             classification = T,
             )

  trees[k] <- rf$num.trees
  error[k] <- rf$prediction.error

  rf <- NULL
  k <- k + 1
}

treesb <- c()
errorb <- c()

k <- 1 # iterador
for (i in c(100,200,300,500,700,800)){

  rf<-ranger(churn~.,
             data = data_training_b,
             num.trees = i,
             mtry = (ncol(data_training_b)-1) %>% sqrt() %>% floor(),
             min.node.size = 1,
             splitrule = "gini",
             classification = T,
             )

  treesb[k] <- rf$num.trees
  errorb[k] <- rf$prediction.error

  rf <- NULL
  k <- k + 1
}

treesc <- c()
errorc <- c()
k <- 1 # iterador
for (i in c(100,200,300,500,700,800)){

  rf<-ranger(churn~.,
             data = data_training_c,
             num.trees = i,
             mtry = (ncol(data_training_c)-1) %>% sqrt() %>% floor(),
             min.node.size = 1,
             splitrule = "gini",
             classification = T,
             )

  treesc[k] <- rf$num.trees
  errorc[k] <- rf$prediction.error

  rf <- NULL
  k <- k + 1
}

Sys.time() -a
stopCluster(cl)

rf <- data.frame(trees,error)
rfb <- data.frame(treesb,errorb)
rfc <- data.frame(treesc,errorc)
rf
rfb
rfc

```
Parece ser que con una $B=200$, el error de predicción ya no parece reducirse mucho.

### 14. Escoge un random forest para hacer las predicciones. Grafica la importancia de las variables. Interpreta

```{r}
# vuelvo a correr el mejor random forest
cl<- detectCores() %>% makeCluster()
cl
best_rf <- ranger(churn~.,
             data = data_training_a,
             num.trees = 200,
             mtry = (ncol(data_training_a)-1) %>% sqrt() %>% floor(),
             importance = "impurity",
             classification = T,
             )
stopCluster(cl)

# Grafica
df_imp <- data.frame(names=(importance(best_rf) %>%
              names()),importance=(importance(best_rf)))


ggplot(df_imp,aes(x=reorder(names,importance),y=importance)) +
  geom_bar(stat="identity") +
  xlab("Variable")+
  ylab("importancia")+
  coord_flip()

```


### 15. Genera las predicciones OOS para el random forest. Guardalas en la misma data.frame que los otros modelos
```{r}
best_rf$predictions


```


### 16 (2pts). Corre el mismo forest pero ahora con `probability = T`. Esto generará predicciones númericas en lugar de categóricas. Genera las predicciones continuas y guardalas en el mismo data frame
```{r}
# vuelvo a correr el mejor random forest, con probability=T
cl<- detectCores() %>% makeCluster()
cl
best_rf <- ranger(churn~.,
             data = data_training_a,
             num.trees = 200,
             mtry = (ncol(data_training_a)-1) %>% sqrt() %>% floor(),
             importance = "impurity",
             classification = T,
             probability = T
             )
stopCluster(cl)

# prediccion continua de churn==1
best_rf$predictions[,1]
```

### 17 (4 pts). Genera graficas de las curvas ROC para los tres modelos. Cual parece ser mejor?
```{r}

```


### 18. Genera una tabla con el AUC ROC. Cuál es el mejor modelo ?
```{r}

```

### 19 (2pts). Escoge un punto de corte para generar predicciones categoricas para el LASSO basado en la Curva ROC. Genera las matrices de confusión para cada modelo. Compáralas. Qué tipo de error es mas pernicioso?
```{r}

```

### 20 (2pts). Finalmente, construye una lift table. Esto es, para 20 grupos del score predecido, genera 1) El promedio de las predicciones, 2) el promedio del churn observado. Existe monotonía? El mejor algoritmo es monotónico? (Tip: usa `ntile` para generar los grupos a partir de las predicciones)
```{r}

```


### 21. Concluye. Que estrategia harías con este modelo? Cómo generarías valor a partir de el?
```{r}

```
